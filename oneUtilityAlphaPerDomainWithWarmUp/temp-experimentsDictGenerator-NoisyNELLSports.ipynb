{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook é baseado no ```./experimentsDictGenerator.ipynb```, porém seu foco está apenas nos experimentos de transferência com dados de origem ruidosos considerando o domínio NELL Sports. Em nossos últimos experimentos, notamos que o NELL Sports é sensível ao ruído adicionado ao domínio de origem e aos hiperparâmetros de nosso método. Porém, nossa metodologia parece apresentar uma falha, pois cada iteração da validação cruzada realiza apenas uma divisão aleatória entre domínios de origem e de destino e gera apenas um ruído. Sendo assim, os resultados podem estar dependentes da divisão e do ruído gerado. Para reduzir esse problema, rodamos novos experimentos com diferentes `randomSeed` a fim gerar novas divisões e ruídos a cada iteração. Isso aumenta significativamente a quantidade de experimentos a serem realizados e esse é o motivo pelo qual focamos inicialmente apenas no NELL Sports. Se for necessário, realizaremos as mesmas análises para os outros experimentos. Para não alterar o que já tá funcionando no notebook ```./experimentsDictGenerator.ipynb```, realizamos essas análises preliminares neste novo notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.experiment import loadDatabase\n",
    "from utils.utils import getHashFromDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/preprocessed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transfer with Noisy Source**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we perform transfer learning from a noisy source to a target domain. To control the noise intensity, we build both target and source sets from the same dataset. This allow us to bypass the challenge of finding a good mapping. Before cobining the source and target data, we randomly add, remove or change the types of the relations on the source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSeeds = list(range(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getExperimentID(experimentDict):\n",
    "    experimentID = getHashFromDict(experimentDict)\n",
    "    return experimentID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonFixedParams = {\n",
    "    \"numberOfClauses\": 8,\n",
    "    \"numberOfCycles\": 100,\n",
    "    \"maxTreeDepth\": 3,\n",
    "    \"nEstimators\": 10,\n",
    "    \"nodeSize\": 2,\n",
    "    \"negPosRatio\": 2,\n",
    "    \"maxFailedNegSamplingRetries\": 50,\n",
    "    \"ignoreSTDOUT\": True,\n",
    "    \"trainNSplits\": 5,\n",
    "    \"trainSourceSplits\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only NELL Sports is handled in this preliminary experiments. We want to check whether the randomness affect our last results or not\n",
    "\n",
    "datasetParams = [\n",
    "    # # NELL Finances\n",
    "    # {\n",
    "    #     \"databasePath\": f\"{DATA_PATH}/nell_finances\",\n",
    "    #     \"targetPredicate\": None, # Default: companyeconomicsector/2\n",
    "    #     \"resetTargetPredicate\": False,      \n",
    "    #     \"useRecursion\": True\n",
    "    # }, \n",
    "\n",
    "    # # Yeast\n",
    "    # {\n",
    "    #     \"databasePath\": f\"{DATA_PATH}/yeast\",\n",
    "    #     \"targetPredicate\": None, # Default: proteinclass/2\n",
    "    #     \"resetTargetPredicate\": False,\n",
    "    #     \"useRecursion\": True\n",
    "    # },\n",
    "\n",
    "    # NELL Sports\n",
    "    {\n",
    "        \"databasePath\": f\"{DATA_PATH}/nell_sports\",\n",
    "        \"targetPredicate\": None, # Default: teamplayssport/2\n",
    "        \"resetTargetPredicate\": False,\n",
    "        \"useRecursion\": True\n",
    "    },\n",
    "\n",
    "    # # Cora\n",
    "    # {\n",
    "    #     \"databasePath\": f\"{DATA_PATH}/cora\",\n",
    "    #     \"targetPredicate\": None, # Default: samevenue/2\n",
    "    #     \"resetTargetPredicate\": False,\n",
    "    #     \"useRecursion\": False\n",
    "    # },\n",
    "\n",
    "    # # UWCSE\n",
    "    # {\n",
    "    #     \"databasePath\": f\"{DATA_PATH}/uwcse\",\n",
    "    #     \"targetPredicate\": None, # Default: advisedby/2\n",
    "    #     \"resetTargetPredicate\": False,\n",
    "    #     \"useRecursion\": False\n",
    "    # },\n",
    "\n",
    "    # # Twitter\n",
    "    # {\n",
    "    #     \"databasePath\": f\"{DATA_PATH}/twitter\",\n",
    "    #     \"targetPredicate\": None, # Default: accounttype/2\n",
    "    #     \"resetTargetPredicate\": False,       \n",
    "    #     \"useRecursion\": True\n",
    "    # },\n",
    "\n",
    "    # # IMDB\n",
    "    # {\n",
    "    #     \"databasePath\": f\"{DATA_PATH}/imdb\",\n",
    "    #     \"targetPredicate\": None, # Default: workedunder/2\n",
    "    #     \"resetTargetPredicate\": False,\n",
    "    #     \"useRecursion\": False\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextModelParams():\n",
    "    utilityAlphaValues = [0, 0.3, 0.6, 1, 1.3]\n",
    "    utilityAlphaList = [\n",
    "        {\n",
    "            \"sourceUtilityAlpha\": sourceAlpha,\n",
    "            \"targetUtilityAlpha\": targetAlpha\n",
    "        } for sourceAlpha, targetAlpha in itertools.product(utilityAlphaValues, utilityAlphaValues)\n",
    "    ]\n",
    "    \n",
    "    utilityAlphaSetIterList = [{\"utilityAlphaSetIter\": iteration} for iteration in [1]]\n",
    "\n",
    "    weightList = [\n",
    "        {\n",
    "            \"weight\": {\n",
    "                \"strategy\": \"balancedInstanceGroupUniform\",\n",
    "                \"parameters\": {\n",
    "                    \"balanceStrength\": 0 # It is equivalent to scalar weighting schema\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"weight\": {\n",
    "                \"strategy\": \"balancedInstanceGroupUniform\",\n",
    "                \"parameters\": {\n",
    "                    \"balanceStrength\": 1\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"weight\": {\n",
    "                \"strategy\": \"balancedInstanceGroupUniform\",\n",
    "                \"parameters\": {\n",
    "                    \"balanceStrength\": 0.5\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    noiseStrengthValues = [(1e-5)*(2**i) for i in range(0, 15)]\n",
    "    noiseStrengthList = [{\"noiseStrength\": strength} for strength in noiseStrengthValues]\n",
    "\n",
    "    paramsGrid = [\n",
    "        {\n",
    "            **utilityParams,\n",
    "            **utilityAlphaSetIterList,\n",
    "            **weightParams, \n",
    "            **noiseStrengthParams\n",
    "        } for utilityParams, utilityAlphaSetIterList, weightParams, noiseStrengthParams in itertools.product(\n",
    "            utilityAlphaList, \n",
    "            utilityAlphaSetIterList,\n",
    "            weightList,\n",
    "            noiseStrengthList\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for params in paramsGrid:\n",
    "        yield params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentSettingJSONsBasePath = \"./experimentSettingJSONs/noisyTransferLearning-onlyNELLSports\"\n",
    "os.makedirs(experimentSettingJSONsBasePath, exist_ok = True)\n",
    "\n",
    "for randomSeed in randomSeeds:\n",
    "    experiments = []\n",
    "    EXPERIMENTS_BASE_PATH = f\"./experiments/noisyTransferLearning-onlyNELLSports/noisyTransferLearning-randomSeed={randomSeed}\"\n",
    "    for params in datasetParams:    \n",
    "        for paramsGrid in getNextModelParams():\n",
    "            experimentDict = {\n",
    "                **commonFixedParams, \n",
    "                **params,\n",
    "                **paramsGrid\n",
    "            }\n",
    "\n",
    "            experimentDict.update({\n",
    "                \"path\": EXPERIMENTS_BASE_PATH,\n",
    "                \"randomSeed\": randomSeed\n",
    "            })\n",
    "            experimentID = getExperimentID(experimentDict)\n",
    "            experimentDict[\"id\"] = experimentID\n",
    "            \n",
    "            experiments.append(experimentDict)\n",
    "\n",
    "    with open(f\"{experimentSettingJSONsBasePath}/experiments-noisyTransferLearning-randomSeed={randomSeed}.json\", \"w\") as f:\n",
    "        json.dump(experiments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
