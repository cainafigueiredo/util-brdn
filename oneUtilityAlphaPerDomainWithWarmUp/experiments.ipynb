{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# FILE_DIR = os.path.dirname(__file__)\n",
    "FILE_DIR = os.path.abspath(\".\")\n",
    "PROJECT_DIR = os.path.abspath(f\"{FILE_DIR}/..\")\n",
    "PREPROCESSED_DATA_PATH = f\"{PROJECT_DIR}/data/preprocessed\"\n",
    "\n",
    "# TODO: Install modified srlearn and BoostSRL as dependencies of this project. After that, I need to remove the following two lines and to change how srlearn modules are being imported in the third line below.\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(f\"{PROJECT_DIR}/../srlearn\")\n",
    "sys.path.append(PROJECT_DIR)\n",
    "from srlearn.database import Database\n",
    "from srlearn.background import Background\n",
    "from srlearn.rdn import RDNBoost, RDNBoostTransferLearning, TreeBoostler\n",
    "from srlearn.weight import WeightFactory\n",
    "from utils.experiment import getLogger, loadDatabase\n",
    "from glob import glob\n",
    "from copy import copy\n",
    "from typing import Optional, Union, Literal\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing.managers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "DATA_PATH = \"../data/preprocessed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Useful Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **General**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning from scratch using the original RDN-Boost\n",
    "def runSingleExperiment_OriginalRDNBoost(\n",
    "    experimentPath: str = \".\", \n",
    "    databaseTrain: Database = None,\n",
    "    databaseTest: Database = None,\n",
    "    nEstimators: int = 10,\n",
    "    nodeSize: int = 2,\n",
    "    maxTreeDepth: int = 3,\n",
    "    negPosRatio: int = 2,\n",
    "    numberOfClauses: int = 8,\n",
    "    numberOfCycles: int = 100,\n",
    "    ignoreSTDOUT: bool = True,\n",
    "    logger: logging.Logger = None\n",
    ") -> dict:\n",
    "    assert databaseTrain is not None\n",
    "    assert databaseTest is not None\n",
    "\n",
    "    if not logger:\n",
    "        logger = getLogger(\"Original RDN-B\", level = logging.DEBUG)\n",
    "\n",
    "    path = os.path.join(experimentPath, \"originalRDNBoost\")\n",
    "    logger.info(\"RUNNING ORIGINAL RDN-B...\")\n",
    "    logger.info(f\"Progress will be store at {path}\")\n",
    "    \n",
    "    model = RDNBoost(\n",
    "        n_estimators = nEstimators, \n",
    "        node_size = nodeSize, \n",
    "        max_tree_depth = maxTreeDepth, \n",
    "        neg_pos_ratio = negPosRatio,\n",
    "        number_of_clauses = numberOfClauses,\n",
    "        number_of_cycles = numberOfCycles,\n",
    "        path = path\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Training the model on the training set...\")\n",
    "    model.fit(databaseTrain, ignoreSTDOUT = ignoreSTDOUT)\n",
    "\n",
    "    logger.info(f\"Evaluating the model on the test set...\")\n",
    "    model._run_inference(databaseTest, ignoreSTDOUT = ignoreSTDOUT)\n",
    "\n",
    "    model._generate_dotimages()\n",
    "    dotImages = model._dotimages\n",
    "    metrics = model._prediction_metrics\n",
    "\n",
    "    result = {\"model\": copy(model), \"treeImages\": copy(dotImages), \"metrics\": copy(metrics)}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning from scratch using our implementation analogous to original RDN-Boost\n",
    "def runSingleExperiment_AnalogousToRDNBoost(\n",
    "    experimentPath: str = \".\", \n",
    "    databaseTrain: Database = None,\n",
    "    databaseTest: Database = None,\n",
    "    nEstimators: int = 10,\n",
    "    nodeSize: int = 2,\n",
    "    maxTreeDepth: int = 3,\n",
    "    negPosRatio: int = 2,\n",
    "    numberOfClauses: int = 8,\n",
    "    numberOfCycles: int = 100,\n",
    "    ignoreSTDOUT: bool = True,\n",
    "    logger: logging.Logger = None\n",
    ") -> dict:\n",
    "    assert databaseTrain is not None\n",
    "    assert databaseTest is not None\n",
    "\n",
    "    if not logger:\n",
    "        logger = getLogger(\"Analogous to RDN-B\", level = logging.DEBUG)\n",
    "\n",
    "    path = os.path.join(experimentPath, \"analogousToOriginalRDNBoost\")\n",
    "    logger.info(\"RUNNING OUR APPROACH ANALOGOUS TO THE ORIGINAL RDN-B...\")\n",
    "    logger.info(f\"Progress will be store at {path}\")\n",
    "\n",
    "    targetDomainTargetRelation = databaseTrain.getTargetRelation()\n",
    "\n",
    "    emptySourceDatabase = Database()\n",
    "    emptySourceDatabase.getTargetRelation = lambda: targetDomainTargetRelation\n",
    "    emptySourceDatabase.modes = databaseTrain.modes\n",
    "\n",
    "    weightFactory = WeightFactory()\n",
    "    weightStrategy = weightFactory.getWeightStrategy(\"scalar\", weight = 1)\n",
    "\n",
    "    database = Database.prepareTransferLearningDatabase(\n",
    "        emptySourceDatabase, \n",
    "        databaseTrain, \n",
    "        weightStrategy = weightStrategy\n",
    "    )\n",
    "\n",
    "    utilityAlpha = 1\n",
    "\n",
    "    model = RDNBoostTransferLearning(\n",
    "        n_estimators = nEstimators,\n",
    "        node_size = nodeSize,\n",
    "        max_tree_depth = maxTreeDepth,\n",
    "        neg_pos_ratio = negPosRatio,\n",
    "        number_of_clauses = numberOfClauses,\n",
    "        number_of_cycles = numberOfCycles,\n",
    "        source_utility_alpha = utilityAlpha,\n",
    "        target_utility_alpha = utilityAlpha,\n",
    "        path = path\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Training the model on the training set...\")\n",
    "    model.fit(database, ignoreSTDOUT = ignoreSTDOUT)\n",
    "\n",
    "    logger.info(f\"Evaluating the model on the test set...\")\n",
    "    model._run_inference(databaseTest, ignoreSTDOUT = ignoreSTDOUT)\n",
    "\n",
    "    model._generate_dotimages()\n",
    "    dotImages = model._dotimages\n",
    "    metrics = model._prediction_metrics\n",
    "\n",
    "    result = {\"model\": copy(model), \"treeImages\": copy(dotImages), \"metrics\": copy(metrics)}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Transfer Learning implementation\n",
    "def runSingleExperiment_TransferLearning(\n",
    "    experimentPath: str = \".\", \n",
    "    sourceDatabase: Database = None,\n",
    "    targetDatabaseTrain: Database = None,\n",
    "    targetDatabaseTest: Database = None,\n",
    "    nEstimators: int = 10,\n",
    "    nodeSize: int = 2,\n",
    "    maxTreeDepth: int = 3,\n",
    "    negPosRatio: int = 2,\n",
    "    numberOfClauses: int = 8,\n",
    "    numberOfCycles: int = 100,\n",
    "    ignoreSTDOUT: bool = True,\n",
    "    useRecursion: bool = False,\n",
    "    randomSeed: int = 10,\n",
    "    maxFailedNegSamplingRetries: int = 50,\n",
    "    weightStrategy: WeightFactory = None,\n",
    "    sourceUtilityAlpha: float = 1,\n",
    "    targetUtilityAlpha: float = 1,\n",
    "    utilityAlphaSetIter: int = 1,\n",
    "    relationMapping: dict = None,\n",
    "    termTypeMapping: dict = None,\n",
    "    logger: logging.Logger = None,\n",
    ") -> dict:\n",
    "    assert sourceDatabase is not None\n",
    "    assert targetDatabaseTrain is not None\n",
    "    assert targetDatabaseTest is not None\n",
    "    assert weightStrategy is not None\n",
    "    assert sourceUtilityAlpha >= 0\n",
    "    assert targetUtilityAlpha >= 0\n",
    "    assert relationMapping is not None\n",
    "    assert termTypeMapping is not None\n",
    "\n",
    "    if not logger:\n",
    "        logger = getLogger(\"Transfer Learning RDN-B\", level = logging.DEBUG)\n",
    "\n",
    "    targetDomainTargetRelation = targetDatabaseTrain.getTargetRelation()\n",
    "\n",
    "    path = os.path.join(experimentPath, \"transferLearning\")\n",
    "    logger.info(\"RUNNING TRANSFER LEARNING...\")\n",
    "    logger.info(f\"Progress will be store at {path}\")\n",
    "\n",
    "    logger.info(\"Mapping source domain to the target domain...\")\n",
    "\n",
    "    sourceTargetRelation = [k for k,v in relationMapping.items() if v == targetDomainTargetRelation][0]\n",
    "    \n",
    "    logger.debug(f\"Relation mapping: {relationMapping}\")\n",
    "    logger.debug(f\"Term type mapping: {termTypeMapping}\")\n",
    "\n",
    "    mappedSourceDatabase = sourceDatabase.setTargetPredicate(\n",
    "        sourceTargetRelation, \n",
    "        useRecursion = useRecursion,\n",
    "        negPosRatio = negPosRatio,\n",
    "        maxFailedNegSamplingRetries = maxFailedNegSamplingRetries\n",
    "    )\n",
    "    mappedSourceDatabase = mappedSourceDatabase.applyMapping(relationMapping, termTypeMapping, \"source\")\n",
    "\n",
    "    logger.info(\"Combining source and target databases...\")\n",
    "\n",
    "    database = Database.prepareTransferLearningDatabase(\n",
    "        mappedSourceDatabase, \n",
    "        targetDatabaseTrain, \n",
    "        weightStrategy = weightStrategy\n",
    "    )\n",
    "\n",
    "    model = RDNBoostTransferLearning(\n",
    "        n_estimators = nEstimators,\n",
    "        node_size = nodeSize,\n",
    "        max_tree_depth = maxTreeDepth,\n",
    "        neg_pos_ratio = negPosRatio,\n",
    "        number_of_clauses = numberOfClauses,\n",
    "        number_of_cycles = numberOfCycles,\n",
    "        source_utility_alpha = sourceUtilityAlpha,\n",
    "        target_utility_alpha = targetUtilityAlpha,\n",
    "        utility_alpha_set_iter = utilityAlphaSetIter,\n",
    "        path = path\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Training the model on the training set...\")\n",
    "    model.fit(database, ignoreSTDOUT = ignoreSTDOUT)\n",
    "\n",
    "    logger.info(f\"Evaluating the model on the test set...\")\n",
    "    model._run_inference(targetDatabaseTest, ignoreSTDOUT = ignoreSTDOUT)\n",
    "\n",
    "    model._generate_dotimages()\n",
    "    dotImages = model._dotimages\n",
    "    metrics = model._prediction_metrics\n",
    "\n",
    "    result = {\"model\": copy(model), \"treeImages\": copy(dotImages), \"metrics\": copy(metrics)}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Transfer Learning implementation\n",
    "def runSingleExperiment_TreeBoostler(\n",
    "    experimentPath: str = \".\", \n",
    "    sourceDatabase: Database = None,\n",
    "    targetDatabaseTrain: Database = None,\n",
    "    targetDatabaseTest: Database = None,\n",
    "    nEstimators: int = 10,\n",
    "    nodeSize: int = 2,\n",
    "    maxTreeDepth: int = 3,\n",
    "    negPosRatio: int = 2,\n",
    "    numberOfClauses: int = 8,\n",
    "    numberOfCycles: int = 100,\n",
    "    ignoreSTDOUT: bool = True,\n",
    "    searchArgPermutation: bool = True,\n",
    "    allowSameTargetMap: bool = False,\n",
    "    refine: bool = True,\n",
    "    maxRevisionIterations: int = 1,\n",
    "    logger: logging.Logger = None,\n",
    ") -> dict:\n",
    "    assert sourceDatabase is not None\n",
    "    assert targetDatabaseTrain is not None\n",
    "    assert targetDatabaseTest is not None\n",
    "\n",
    "    if not logger:\n",
    "        logger = getLogger(\"TreeBoostler\", level = logging.DEBUG)\n",
    "\n",
    "    path = os.path.join(experimentPath, \"treeBoostler\")\n",
    "    logger.info(\"RUNNING TREEBOOSTLER...\")\n",
    "    logger.info(f\"Progress will be store at {path}\")\n",
    "    \n",
    "    model = TreeBoostler(\n",
    "        searchArgPermutation = searchArgPermutation,\n",
    "        allowSameTargetMap = allowSameTargetMap,\n",
    "        refine = refine,\n",
    "        maxRevisionIterations = maxRevisionIterations,\n",
    "        n_estimators = nEstimators,\n",
    "        node_size = nodeSize,\n",
    "        max_tree_depth = maxTreeDepth,\n",
    "        neg_pos_ratio = negPosRatio,\n",
    "        number_of_clauses = numberOfClauses,\n",
    "        number_of_cycles = numberOfCycles,\n",
    "        path = path\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Training the model on the training set...\")\n",
    "    model.fit(sourceDatabase, targetDatabaseTrain, ignoreSTDOUT = ignoreSTDOUT)\n",
    "\n",
    "    logger.info(f\"Evaluating the model on the test set...\")\n",
    "    model._run_inference(targetDatabaseTest, ignoreSTDOUT = ignoreSTDOUT)\n",
    "\n",
    "    model._generate_dotimages()\n",
    "    dotImages = model._dotimages\n",
    "    metrics = model._prediction_metrics\n",
    "\n",
    "    result = {\"model\": copy(model), \"treeImages\": copy(dotImages), \"metrics\": copy(metrics)}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentResultSummarization(experimentResult: dict, logger: logging.Logger = None, ):\n",
    "    if not logger:\n",
    "        logger = getLogger(\"Result summarization\")\n",
    "\n",
    "    logger.info(\"Extracting performance metrics from experiment results:\")\n",
    "    metrics = {}\n",
    "    for exp, expResults in experimentResult.items():\n",
    "        metrics[exp] = metrics.get(exp, {})\n",
    "        for trainFold, foldResults in expResults.items():\n",
    "            for metricName, metricValue in foldResults[\"metrics\"].items():\n",
    "                metrics[exp][metricName] = metrics[exp].get(metricName, [])\n",
    "                metrics[exp][metricName] += [float(metricValue)]\n",
    "\n",
    "    for exp, expMetrics in metrics.items():\n",
    "        for metricName, metricValues in expMetrics.items():\n",
    "            metricValues = np.array(metricValues)\n",
    "            mean = metricValues.mean()\n",
    "            std = metricValues.std()\n",
    "            logger.info(f\"{exp}: {metricName} = {mean:.4f} +- {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cross-Validation for Transfer Setting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It assumes that there is not enough target data for learning and resorts to a related domain (source) to augment the training data. To simulate low target data availability, each iteration of our cross validation for transfer settings selects one fold for training and the remaining for test. This is the opposite of traditional cross validation. \n",
    "\n",
    "In our transfer experiments, we also consider learning from scratch. In this case, the learning only relies on the limited target data, as simulated by our cross validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAllFolds(experimentDict: dict, logger = None) -> dict:\n",
    "    \"\"\"Given a dict specifying the experiment setting, it runs an experiment similar to k-fold cross validation. The difference is that, at each iteration, only one fold is used for training while the remaining k-1 are used for testing. The is because we simulate a scenario where there is only a few data on the target domain. It returns a dict with all the results.\"\"\"\n",
    "\n",
    "    experiment = experimentDict\n",
    "    experimentID = experiment[\"id\"]\n",
    "    experimentBasePath = os.path.join(experiment[\"path\"], experimentID)\n",
    "\n",
    "    os.makedirs(experimentBasePath, exist_ok = True)\n",
    "\n",
    "    with open(os.path.join(experimentBasePath, \"setting.json\"), \"w\") as f:\n",
    "        json.dump(experiment, f)\n",
    "\n",
    "    if not logger:\n",
    "        logger = getLogger(experimentID, level = logging.DEBUG)\n",
    "\n",
    "    logger.info(\"Parsing experiment parameters...\")\n",
    "\n",
    "    useRecursion = experiment.get(\"useRecursion\", False)\n",
    "    negPosRatio = experiment.get(\"negPosRatio\", 1)\n",
    "    randomSeed = experiment.get(\"randomSeed\", 10)\n",
    "    maxFailedNegSamplingRetries = experiment.get(\"maxFailedNegSamplingRetries\", 50)\n",
    "    numberOfClauses = experiment.get(\"numberOfClauses\", 8)\n",
    "    numberOfCycles = experiment.get(\"numberOfCycles\", 100)\n",
    "    maxTreeDepth = experiment.get(\"maxTreeDepth\", 3)\n",
    "    nEstimators = experiment.get(\"nEstimators\", 10)\n",
    "    nodeSize = experiment.get(\"nodeSize\", 2)\n",
    "    sourceUtilityAlpha = experiment.get(\"sourceUtilityAlpha\", 1)\n",
    "    targetUtilityAlpha = experiment.get(\"targetUtilityAlpha\", 1)\n",
    "    utilityAlphaSetIter = experiment.get(\"utilityAlphaSetIter\", 1)\n",
    "    runOriginalRDNBoost = experiment.get(\"runOriginalRDNBoost\", False)\n",
    "    runTransferLearning = experiment.get(\"runTransferLearning\", False)\n",
    "    runAnalogousToOriginalRDNBoost = experiment.get(\"runAnalogousToOriginalRDNBoost\", False)\n",
    "    runTreeBoostler = experiment.get(\"runTreeBoostler\", False)\n",
    "    ignoreSTDOUT = experiment.get(\"ignoreSTDOUT\", False)\n",
    "    searchArgPermutation = experiment.get(\"searchArgPermutation\", True)\n",
    "    allowSameTargetMap = experiment.get(\"allowSameTargetMap\", False)\n",
    "    refine = experiment.get(\"refine\", True)\n",
    "    maxRevisionIterations = experiment.get(\"maxRevisionIterations\", 1)\n",
    "\n",
    "    anyModelIsSet = runOriginalRDNBoost or runTransferLearning or runAnalogousToOriginalRDNBoost or runTreeBoostler\n",
    "    assert anyModelIsSet, \"No model to run. `runOriginalRDNBoost`, `runTransferLearning`, and `analogousToOriginalRDNBoost` can not be set to False simultaneously.\"\n",
    "\n",
    "    if runTransferLearning or runTreeBoostler:\n",
    "        logger.info(\"Loading source database...\")\n",
    "        \n",
    "        sourceDatabase = loadDatabase(\n",
    "            folds = None, \n",
    "            useRecursion = useRecursion, \n",
    "            logger = logger,\n",
    "            negPosRatio = negPosRatio,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            **experiment[\"sourceDatabase\"]\n",
    "        )\n",
    "\n",
    "        if runTransferLearning:\n",
    "            weightFactory = WeightFactory()\n",
    "            weightStrategy = weightFactory.getWeightStrategy(experiment[\"weight\"][\"strategy\"], **experiment[\"weight\"][\"parameters\"])\n",
    "        \n",
    "            relationMapping = experiment[\"mapping\"][\"relationMapping\"]\n",
    "            termTypeMapping = experiment[\"mapping\"][\"termTypeMapping\"]\n",
    "\n",
    "    targetDatabasePath = experiment[\"targetDatabase\"][\"path\"]\n",
    "    allTargetFolds = [os.path.basename(path) for path in glob(f\"{targetDatabasePath}/fold*\")]\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for fold in allTargetFolds:\n",
    "        experimentFoldPath = os.path.join(experimentBasePath, fold)\n",
    "        os.makedirs(experimentFoldPath, exist_ok = True)\n",
    "\n",
    "        logger.info(f\"RUNNING EXPERIMENTS USING {fold.upper()} AS TRAINING FOLD...\")\n",
    "\n",
    "        targetDatabaseTrainFold = fold\n",
    "        targetDatabaseTestFolds = list(set(allTargetFolds) - set([targetDatabaseTrainFold]))\n",
    "\n",
    "        logger.info(\"Loading target database for training...\")\n",
    "        logger.debug(f\"Train fold: {targetDatabaseTrainFold}\")\n",
    "\n",
    "        targetDatabaseTrain = loadDatabase(\n",
    "            folds = [targetDatabaseTrainFold], \n",
    "            useRecursion = useRecursion, \n",
    "            logger = logger,\n",
    "            negPosRatio = negPosRatio,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            **experiment[\"targetDatabase\"]\n",
    "        )\n",
    "\n",
    "        logger.info(\"Loading target database for testing...\")\n",
    "        logger.debug(f\"Test folds: {targetDatabaseTestFolds}\")\n",
    "\n",
    "        targetDatabaseTest = loadDatabase(\n",
    "            folds = targetDatabaseTestFolds, \n",
    "            useRecursion = useRecursion, \n",
    "            logger = logger,\n",
    "            negPosRatio = negPosRatio,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            **experiment[\"targetDatabase\"]\n",
    "        )\n",
    "\n",
    "        targetDomainTargetRelation = targetDatabaseTrain.getTargetRelation()\n",
    "\n",
    "        logger.debug(f\"Target relation for target database: {targetDomainTargetRelation}\")\n",
    "\n",
    "        if runOriginalRDNBoost:\n",
    "            result[\"originalRDNBoost\"] = result.get(\"originalRDNBoost\", {})\n",
    "            result[\"originalRDNBoost\"][targetDatabaseTrainFold] = runSingleExperiment_OriginalRDNBoost(\n",
    "                experimentPath = experimentFoldPath, \n",
    "                databaseTrain = targetDatabaseTrain,\n",
    "                databaseTest = targetDatabaseTest,\n",
    "                nEstimators = nEstimators,\n",
    "                nodeSize = nodeSize,\n",
    "                maxTreeDepth = maxTreeDepth,\n",
    "                negPosRatio = negPosRatio,\n",
    "                numberOfClauses = numberOfClauses,\n",
    "                numberOfCycles = numberOfCycles,\n",
    "                ignoreSTDOUT = ignoreSTDOUT,\n",
    "                logger = logger        \n",
    "            )\n",
    "\n",
    "        if runAnalogousToOriginalRDNBoost:\n",
    "            result[\"analogousToOriginalRDNBoost\"] = result.get(\"analogousToOriginalRDNBoost\", {})\n",
    "            result[\"analogousToOriginalRDNBoost\"][targetDatabaseTrainFold] = runSingleExperiment_AnalogousToRDNBoost(\n",
    "                experimentPath = experimentFoldPath, \n",
    "                databaseTrain = targetDatabaseTrain,\n",
    "                databaseTest = targetDatabaseTest,\n",
    "                nEstimators = nEstimators,\n",
    "                nodeSize = nodeSize,\n",
    "                maxTreeDepth = maxTreeDepth,\n",
    "                negPosRatio = negPosRatio,\n",
    "                numberOfClauses = numberOfClauses,\n",
    "                numberOfCycles = numberOfCycles,\n",
    "                ignoreSTDOUT = ignoreSTDOUT,\n",
    "                logger = logger        \n",
    "            )\n",
    "\n",
    "        if runTransferLearning:\n",
    "            result[\"transferLearning\"] = result.get(\"transferLearning\", {})\n",
    "            result[\"transferLearning\"][targetDatabaseTrainFold] = runSingleExperiment_TransferLearning(\n",
    "                experimentPath = experimentFoldPath,\n",
    "                sourceDatabase = sourceDatabase,\n",
    "                targetDatabaseTrain = targetDatabaseTrain,\n",
    "                targetDatabaseTest = targetDatabaseTest,\n",
    "                nEstimators = nEstimators,\n",
    "                nodeSize = nodeSize,\n",
    "                maxTreeDepth = maxTreeDepth,\n",
    "                negPosRatio = negPosRatio,\n",
    "                numberOfClauses = numberOfClauses,\n",
    "                numberOfCycles = numberOfCycles,\n",
    "                ignoreSTDOUT = ignoreSTDOUT,\n",
    "                useRecursion = useRecursion,\n",
    "                randomSeed = randomSeed,\n",
    "                maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "                weightStrategy = weightStrategy,\n",
    "                sourceUtilityAlpha = sourceUtilityAlpha,\n",
    "                targetUtilityAlpha = targetUtilityAlpha,\n",
    "                utilityAlphaSetIter = utilityAlphaSetIter,\n",
    "                relationMapping = relationMapping,\n",
    "                termTypeMapping = termTypeMapping,\n",
    "                logger = logger,\n",
    "            )\n",
    "\n",
    "        if runTreeBoostler:\n",
    "            result[\"treeBoostler\"] = result.get(\"treeBoostler\", {})\n",
    "            result[\"treeBoostler\"][targetDatabaseTrainFold] = runSingleExperiment_TreeBoostler(\n",
    "                experimentPath = experimentFoldPath,\n",
    "                sourceDatabase = sourceDatabase,\n",
    "                targetDatabaseTrain = targetDatabaseTrain,\n",
    "                targetDatabaseTest = targetDatabaseTest,\n",
    "                nEstimators = nEstimators,\n",
    "                nodeSize = nodeSize,\n",
    "                maxTreeDepth = maxTreeDepth,\n",
    "                negPosRatio = negPosRatio,\n",
    "                numberOfClauses = numberOfClauses,\n",
    "                numberOfCycles = numberOfCycles,\n",
    "                ignoreSTDOUT = ignoreSTDOUT,\n",
    "                searchArgPermutation = searchArgPermutation,\n",
    "                allowSameTargetMap = allowSameTargetMap,\n",
    "                refine = refine,\n",
    "                maxRevisionIterations = maxRevisionIterations,\n",
    "                logger = logger,\n",
    "            )\n",
    "\n",
    "    metricsJSONPath = os.path.join(experimentBasePath, \"metrics.json\")\n",
    "    logger.info(f\"Storing performance metrics at {metricsJSONPath}.\")\n",
    "\n",
    "    allMetrics = {}\n",
    "    for model, folds in result.items():\n",
    "        allMetrics[model] = {fold: foldResults[\"metrics\"] for fold, foldResults in folds.items()}\n",
    "\n",
    "    with open(metricsJSONPath, \"w\") as f:\n",
    "        json.dump(allMetrics, f)\n",
    "\n",
    "    logger.info(\"Experiment has been finished.\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4032"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = {}\n",
    "with open(\"experiments-transferCrossValidation.json\") as f:\n",
    "    experiments = json.load(f)\n",
    "totalExperiments = len(experiments)\n",
    "totalExperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Transfer Learning Experiments: 4000\n",
      "Total Original RDN-Boost Experiments: 8\n",
      "Total Analogous to Original RDN-Boost Experiments: 8\n",
      "Total TreeBoostler Experiments: 16\n"
     ]
    }
   ],
   "source": [
    "transferLearningExperiments = [experimentDict for experimentDict in experiments if \"runTransferLearning\" in experimentDict]\n",
    "originalRDNBoostExperiments = [experimentDict for experimentDict in experiments if \"runOriginalRDNBoost\" in experimentDict]\n",
    "analogousToOriginalRDNBoostExperiments = [experimentDict for experimentDict in experiments if \"runAnalogousToOriginalRDNBoost\" in experimentDict]\n",
    "treeBoostlerExperiments = [experimentDict for experimentDict in experiments if \"runTreeBoostler\" in experimentDict]\n",
    "print(\"Total Transfer Learning Experiments:\", len(transferLearningExperiments))\n",
    "print(\"Total Original RDN-Boost Experiments:\", len(originalRDNBoostExperiments))\n",
    "print(\"Total Analogous to Original RDN-Boost Experiments:\", len(analogousToOriginalRDNBoostExperiments))\n",
    "print(\"Total TreeBoostler Experiments:\", len(treeBoostlerExperiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can be leveraged to prioritize experiments.\n",
    "def skipExperiment(experimentDict):\n",
    "    # The experiment has already been carried out.\n",
    "    experimentID = experimentDict[\"id\"]\n",
    "    experimentPath = experimentDict['path']\n",
    "    if os.path.exists(f\"{experimentPath}/{experimentID}/metrics.json\"):\n",
    "        return True, \"The experiment has already been carried out.\"\n",
    "\n",
    "    # Model filtering [Uncomment the models whose experiments you would like to run]\n",
    "    # =============================================================================================\n",
    "\n",
    "    # 1) It is not an experiment from our transfer learning approach.\n",
    "    if not \"runTransferLearning\" in experimentDict:\n",
    "        return True, \"It is not an experiment from our transfer learning approach.\"\n",
    "\n",
    "    # # 2) It is not an experiment from our approach equivalent to original RDN-Boost.\n",
    "    # if not \"runAnalogousToOriginalRDNBoost\" in experimentDict:\n",
    "    #     return True, \"It is not an experiment from our approach equivalent to original RDN-Boost.\"\n",
    "\n",
    "    # # 3) It is not an experiment from original RDN-Boost.\n",
    "    # if not \"runOriginalRDNBoost\" in experimentDict:\n",
    "    #     return True, \"It is not an experiment from original RDN-Boost.\"\n",
    "\n",
    "    # # 4) It is not an experiment from TreeBoostler.\n",
    "    # if not \"runTreeBoostler\" in experimentDict:\n",
    "    #     return True, \"It is not an experiment from TreeBoostler\"\n",
    "\n",
    "    # ============================================================================================\n",
    "\n",
    "    # Run only transfer experiment from imdb to cora\n",
    "    sourceDomain = os.path.basename(experimentDict[\"sourceDatabase\"][\"path\"])\n",
    "    targetDomain = os.path.basename(experimentDict[\"targetDatabase\"][\"path\"])\n",
    "\n",
    "    if sourceDomain != \"imdb\" or targetDomain != \"uwcse\":\n",
    "        return True, \"It is not a transfer from IMDB to Cora\"\n",
    "\n",
    "    # # Run only transfer with utilityAlphaSetIter != 1 (we already have results for this setting)\n",
    "    # if experimentDict[\"utilityAlphaSetIter\"] == 1:\n",
    "    #     return True, \"UtilityAlphaSetIter is equal to 1.\"\n",
    "\n",
    "    # =============================================================================================\n",
    "    \n",
    "    # # It is a trivial experiment, i.e., it usually achieves very good performances in related work.\n",
    "    # sourceDomain = os.path.basename(experimentDict[\"sourceDatabase\"][\"path\"])\n",
    "    # targetDomain = os.path.basename(experimentDict[\"targetDatabase\"][\"path\"])\n",
    "\n",
    "    # if sourceDomain == \"cora\" and targetDomain == \"imdb\":\n",
    "    #     return True, \"Cora to IMDB transferring usually achieves very good performances in related work.\"\n",
    "\n",
    "    # if sourceDomain == \"yeast\" and targetDomain == \"twitter\":\n",
    "    #     return True, \"Yeast to Twitter transferring usually achieves very good performances in related work.\"\n",
    "\n",
    "    # if sourceDomain == \"uwcse\" and targetDomain == \"imdb\":\n",
    "    #     return True, \"USCSE to IMDB transferring usually achieves very good performances in related work.\"\n",
    "\n",
    "    # if sourceDomain == \"nell_finances\" and targetDomain == \"nell_sports\":\n",
    "    #     return True, \"NELL Finances to NELL Sports transferring usually achieves very good performances in related work.\"\n",
    "\n",
    "    return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 1 # An int greater or equal to 1\n",
    "skippedExperiments = []\n",
    "experimentsToRun = []\n",
    "numProcesses = 3\n",
    "\n",
    "for i, experimentDict in enumerate(experiments[start-1:], start = start):\n",
    "    # TODO: I am temporarily ignoring other models. I  need to further run the other experiments\n",
    "    shouldSkipExperiment, skipMessage = skipExperiment(experimentDict)\n",
    "    experimentID = experimentDict[\"id\"]\n",
    "    if shouldSkipExperiment:\n",
    "        skippedExperiments.append((experimentID, skipMessage))\n",
    "    else:\n",
    "        os.system(f\"rm -rf {experimentDict['path']}/{experimentID}\")\n",
    "        experimentsToRun.append(experimentDict)\n",
    "\n",
    "len(experimentsToRun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsRunningMode = \"parallel\" # Either \"parallel\" or \"sequential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment 2b679b4698cbc49ea7cb74ba976208d4869c19eb7fd756fa1ecf4f64e2b568b4...\n",
      "Starting experiment 1c0d36fc64e2507a4d9c187f5c220e1a6fcd30a03bf87b3eb020f0fa29b3cbaa...\n",
      "Starting experiment af5270bc9334600ae7ba34f8d1bc9f8318f6e69cab92101fd470b0abd54fc4e8...\n",
      "Starting experiment ec008509c3aba5e2d2648d44ef72526927fd719d8c7770ce01996eb97acd071d..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting experiment ff22846a48d94db7addc85cfc3527f8e39d23989678a6f9f11e762d591267925...\n",
      "Starting experiment 107d99fc81f6624e775a345ec457327e6faf8a326f5073c7db26d1b0329c0a26...\n",
      "Starting experiment 4a3e5204395480bd1d0411fe7961ea2bbc379bc28518d8ce2a88e6bd4bf2b452...\n",
      "Starting experiment 4c15fef6886ffa04f1d7322fd9fb74e7916f4c542f24b354a79113abc9a04e04...\n",
      "Experiment finished successfully: ff22846a48d94db7addc85cfc3527f8e39d23989678a6f9f11e762d591267925...\n",
      "Starting experiment 7d69218902a372cedefca661b119cd56b89f3b3b2bba15386cba690ec0efa3fe...\n",
      "Experiment finished successfully: 107d99fc81f6624e775a345ec457327e6faf8a326f5073c7db26d1b0329c0a26...\n",
      "Experiment finished successfully: 7d69218902a372cedefca661b119cd56b89f3b3b2bba15386cba690ec0efa3fe...\n",
      "Experiment finished successfully: 4c15fef6886ffa04f1d7322fd9fb74e7916f4c542f24b354a79113abc9a04e04...\n",
      "Experiment finished successfully: ec008509c3aba5e2d2648d44ef72526927fd719d8c7770ce01996eb97acd071d...\n",
      "Experiment finished successfully: 4a3e5204395480bd1d0411fe7961ea2bbc379bc28518d8ce2a88e6bd4bf2b452...\n",
      "Experiment finished successfully: 2b679b4698cbc49ea7cb74ba976208d4869c19eb7fd756fa1ecf4f64e2b568b4...\n",
      "Experiment finished successfully: 1c0d36fc64e2507a4d9c187f5c220e1a6fcd30a03bf87b3eb020f0fa29b3cbaa...\n",
      "Experiment finished successfully: af5270bc9334600ae7ba34f8d1bc9f8318f6e69cab92101fd470b0abd54fc4e8...\n"
     ]
    }
   ],
   "source": [
    "# Parallel execution of the experiments. \n",
    "# TODO: We get a Kernel Crash and it occurs only when we import the models from srlearn.rdn under Python 3.8.10. In our tests, this problem is solved when running over Python 3.10.5, but the reason why it does not works under Python 3.8.10 is still unknown.\n",
    " \n",
    "if experimentsRunningMode == \"parallel\":\n",
    "    def safePrint(message, consoleOutputLock: multiprocessing.managers.AcquirerProxy):\n",
    "        consoleOutputLock.acquire()\n",
    "        print(message)\n",
    "        consoleOutputLock.release()\n",
    "\n",
    "    def experimentWorker(experimentDict: dict, consoleOutputLock: multiprocessing.managers.AcquirerProxy):\n",
    "        experimentID = experimentDict[\"id\"]\n",
    "        safePrint(f\"Starting experiment {experimentID}...\", consoleOutputLock)\n",
    "        try:\n",
    "            experimentPath = f\"{experimentDict['path']}/{experimentID}\"\n",
    "            os.makedirs(experimentPath, exist_ok = True)    \n",
    "            logger = getLogger(experimentID, logFile = f\"{experimentPath}/experiment.log\", consoleOutput = False)\n",
    "            logger.info(f\"RUNNING EXPERIMENT {i}/{totalExperiments}...\")\n",
    "            experimentResult = runAllFolds(experimentDict, logger = logger)\n",
    "            experimentResultSummarization(experimentResult, logger = logger)\n",
    "            safePrint(f\"Experiment finished successfully: {experimentID}...\", consoleOutputLock)\n",
    "        except Exception as e:\n",
    "            safePrint(f\"The following exception was raised while running the experiment {experimentID}: {e}. Check the logs in the experiment directory for more details.\", consoleOutputLock)\n",
    "            raise e\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers = 8) as p:\n",
    "        with multiprocessing.Manager() as manager:\n",
    "            consoleOutputLock = manager.Lock()\n",
    "            futures = p.map(experimentWorker, experimentsToRun, [consoleOutputLock for experiment in experimentsToRun])\n",
    "            for result in futures:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential execution of the experiments. \n",
    "if experimentsRunningMode == \"sequential\":\n",
    "    totalExperimentsToRun = len(experimentsToRun)\n",
    "    for i, experimentDict in enumerate(experimentsToRun, start = 1):\n",
    "        experimentID = experimentDict[\"id\"]\n",
    "        experimentPath = f\"{experimentDict['path']}/{experimentID}\"\n",
    "        os.makedirs(experimentPath, exist_ok = True)    \n",
    "        logger = getLogger(experimentID, logFile = f\"{experimentPath}/experiment.log\")\n",
    "        logger.info(f\"RUNNING EXPERIMENT {i}/{totalExperimentsToRun}...\")\n",
    "        experimentResult = runAllFolds(experimentDict, logger = logger)\n",
    "        experimentResultSummarization(experimentResult, logger = logger)\n",
    "        clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skippedExperiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Traditional Cross-Validation (no transfer)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It consists of performing traditional k-fold cross validation on the target data. In other words, we consider learning from scratch with enough target data for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTraditionalCrossValidation(experimentDict: dict, logger = None) -> dict:\n",
    "    \"\"\"Given a dictionary specifying the experiment setup, it performs a k-fold cross-validation considering only the data from the target domain without simulating a scenario where there is little data available. It performs learning from scratch using all available target data and using the original RDN-Boost model.\"\"\"\n",
    "\n",
    "    experiment = experimentDict\n",
    "    experimentID = experiment[\"id\"]\n",
    "    experimentBasePath = os.path.join(experiment[\"path\"], experimentID)\n",
    "\n",
    "    os.makedirs(experimentBasePath, exist_ok = True)\n",
    "\n",
    "    with open(os.path.join(experimentBasePath, \"setting.json\"), \"w\") as f:\n",
    "        json.dump(experiment, f)\n",
    "\n",
    "    if not logger:\n",
    "        logger = getLogger(experimentID, level = logging.DEBUG)\n",
    "\n",
    "    logger.info(\"Parsing experiment parameters...\")\n",
    "\n",
    "    useRecursion = experiment.get(\"useRecursion\", False)\n",
    "    negPosRatio = experiment.get(\"negPosRatio\", 1)\n",
    "    randomSeed = experiment.get(\"randomSeed\", 10)\n",
    "    maxFailedNegSamplingRetries = experiment.get(\"maxFailedNegSamplingRetries\", 50)\n",
    "    numberOfClauses = experiment.get(\"numberOfClauses\", 8)\n",
    "    numberOfCycles = experiment.get(\"numberOfCycles\", 100)\n",
    "    maxTreeDepth = experiment.get(\"maxTreeDepth\", 3)\n",
    "    nEstimators = experiment.get(\"nEstimators\", 10)\n",
    "    nodeSize = experiment.get(\"nodeSize\", 2)\n",
    "    ignoreSTDOUT = experiment.get(\"ignoreSTDOUT\", False)\n",
    "    resetTargetPredicate = experiment.get(\"resetTargetPredicate\", False)\n",
    "    targetPredicate = experiment.get(\"targetPredicate\", None)\n",
    "\n",
    "    databasePath = experiment[\"databasePath\"]\n",
    "    allFolds = [os.path.basename(path) for path in glob(f\"{databasePath}/fold*\")]\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for fold in allFolds:\n",
    "        experimentFoldPath = os.path.join(experimentBasePath, fold)\n",
    "        os.makedirs(experimentFoldPath, exist_ok = True)\n",
    "\n",
    "        logger.info(f\"RUNNING EXPERIMENTS USING {fold.upper()} AS TEST FOLD...\")\n",
    "\n",
    "        databaseTestFold = fold\n",
    "        databaseTrainFolds = list(set(allFolds) - set([databaseTestFold]))\n",
    "\n",
    "        logger.info(\"Loading database for training...\")\n",
    "        logger.debug(f\"Train folds: {databaseTrainFolds}\")\n",
    "\n",
    "        databaseTrain = loadDatabase(\n",
    "            path = databasePath,\n",
    "            folds = databaseTrainFolds, \n",
    "            useRecursion = useRecursion, \n",
    "            logger = logger,\n",
    "            negPosRatio = negPosRatio,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            resetTargetPredicate = resetTargetPredicate,\n",
    "            targetPredicate = targetPredicate\n",
    "        )\n",
    "\n",
    "        logger.info(\"Loading database for testing...\")\n",
    "        logger.debug(f\"Test fold: {databaseTestFold}\")\n",
    "\n",
    "        databaseTest = loadDatabase(\n",
    "            path = databasePath,\n",
    "            folds = [databaseTestFold], \n",
    "            useRecursion = useRecursion, \n",
    "            logger = logger,\n",
    "            negPosRatio = negPosRatio,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            resetTargetPredicate = resetTargetPredicate,\n",
    "            targetPredicate = targetPredicate\n",
    "        )\n",
    "\n",
    "        targetRelation = databaseTrain.getTargetRelation()\n",
    "\n",
    "        logger.debug(f\"Target relation for database: {targetRelation}\")\n",
    "\n",
    "        result[databaseTestFold] = runSingleExperiment_OriginalRDNBoost(\n",
    "            experimentPath = experimentFoldPath, \n",
    "            databaseTrain = databaseTrain,\n",
    "            databaseTest = databaseTest,\n",
    "            nEstimators = nEstimators,\n",
    "            nodeSize = nodeSize,\n",
    "            maxTreeDepth = maxTreeDepth,\n",
    "            negPosRatio = negPosRatio,\n",
    "            numberOfClauses = numberOfClauses,\n",
    "            numberOfCycles = numberOfCycles,\n",
    "            ignoreSTDOUT = ignoreSTDOUT,\n",
    "            logger = logger        \n",
    "        )\n",
    "\n",
    "    metricsJSONPath = os.path.join(experimentBasePath, \"metrics.json\")\n",
    "    logger.info(f\"Storing performance metrics at {metricsJSONPath}.\")\n",
    "\n",
    "    allMetrics = {}\n",
    "    for folds, foldResults in result.items():\n",
    "        allMetrics[fold] = foldResults[\"metrics\"]\n",
    "\n",
    "    with open(metricsJSONPath, \"w\") as f:\n",
    "        json.dump(allMetrics, f)\n",
    "\n",
    "    logger.info(\"Experiment has been finished.\")\n",
    "        \n",
    "    return {\"RDNBoost\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = {}\n",
    "with open(\"experiments-noTransferCrossValidation.json\") as f:\n",
    "    experiments = json.load(f)\n",
    "totalExperiments = len(experiments)\n",
    "totalExperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can be leveraged to prioritize experiments.\n",
    "def skipExperiment(experimentDict):\n",
    "    # The experiment has already been carried out.\n",
    "    experimentID = experimentDict[\"id\"]\n",
    "    experimentPath = experimentDict['path']\n",
    "    if os.path.exists(f\"{experimentPath}/{experimentID}/metrics.json\"):\n",
    "        return True, \"The experiment has already been carried out.\"\n",
    "    return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 1 # An int greater or equal to 1\n",
    "skippedExperiments = []\n",
    "experimentsToRun = []\n",
    "numProcesses = 3\n",
    "\n",
    "for i, experimentDict in enumerate(experiments[start-1:], start = start):\n",
    "    # TODO: I am temporarily ignoring other models. I  need to further run the other experiments\n",
    "    shouldSkipExperiment, skipMessage = skipExperiment(experimentDict)\n",
    "    experimentID = experimentDict[\"id\"]\n",
    "    if shouldSkipExperiment:\n",
    "        skippedExperiments.append((experimentID, skipMessage))\n",
    "    else:\n",
    "        experimentsToRun.append(experimentDict)\n",
    "\n",
    "len(experimentsToRun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsRunningMode = \"parallel\" # Either \"parallel\" or \"sequential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel execution of the experiments. \n",
    "# TODO: We get a Kernel Crash and it occurs only when we import the models from srlearn.rdn under Python 3.8.10. In our tests, this problem is solved when running over Python 3.10.5, but the reason why it does not works under Python 3.8.10 is still unknown.\n",
    " \n",
    "if experimentsRunningMode == \"parallel\":\n",
    "    def safePrint(message, consoleOutputLock: multiprocessing.managers.AcquirerProxy):\n",
    "        consoleOutputLock.acquire()\n",
    "        print(message)\n",
    "        consoleOutputLock.release()\n",
    "\n",
    "    def experimentWorker(experimentDict: dict, consoleOutputLock: multiprocessing.managers.AcquirerProxy):\n",
    "        experimentID = experimentDict[\"id\"]\n",
    "        safePrint(f\"Starting experiment {experimentID}...\", consoleOutputLock)\n",
    "        try:\n",
    "            experimentPath = f\"{experimentDict['path']}/{experimentID}\"\n",
    "            os.makedirs(experimentPath, exist_ok = True)    \n",
    "            logger = getLogger(experimentID, logFile = f\"{experimentPath}/experiment.log\", consoleOutput = False)\n",
    "            logger.info(f\"RUNNING EXPERIMENT {i}/{totalExperiments}...\")\n",
    "            experimentResult = runTraditionalCrossValidation(experimentDict, logger = logger)\n",
    "            experimentResultSummarization(experimentResult, logger = logger)\n",
    "            safePrint(f\"Experiment finished successfully: {experimentID}...\", consoleOutputLock)\n",
    "        except Exception as e:\n",
    "            safePrint(f\"The following exception was raised while running the experiment {experimentID}: {e}. Check the logs in the experiment directory for more details.\", consoleOutputLock)\n",
    "            raise e\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers = 8) as p:\n",
    "        with multiprocessing.Manager() as manager:\n",
    "            consoleOutputLock = manager.Lock()\n",
    "            futures = p.map(experimentWorker, experimentsToRun, [consoleOutputLock for experiment in experimentsToRun])\n",
    "            for result in futures:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential execution of the experiments. \n",
    "if experimentsRunningMode == \"sequential\":\n",
    "    totalExperimentsToRun = len(experimentsToRun)\n",
    "    for i, experimentDict in enumerate(experimentsToRun, start = 1):\n",
    "        experimentID = experimentDict[\"id\"]\n",
    "        experimentPath = f\"{experimentDict['path']}/{experimentID}\"\n",
    "        os.makedirs(experimentPath, exist_ok = True)    \n",
    "        logger = getLogger(experimentID, logFile = f\"{experimentPath}/experiment.log\")\n",
    "        logger.info(f\"RUNNING EXPERIMENT {i}/{totalExperimentsToRun}...\")\n",
    "        experimentResult = runTraditionalCrossValidation(experimentDict, logger = logger)\n",
    "        experimentResultSummarization(experimentResult, logger = logger)\n",
    "        clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Progressive Target Data Availability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It evaluates how target data availability impacts the performance of our instance-based transfer learning model. We only consider the best settings for each pair of source and target domains, according to the results from the cross validation for transfer settings. In particular, we define the best setting based on the AUC PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentResultSummarization(experimentResult: dict, logger: logging.Logger = None, ):\n",
    "    if not logger:\n",
    "        logger = getLogger(\"Result summarization\")\n",
    "\n",
    "    logger.info(\"Extracting performance metrics from experiment results:\")\n",
    "    \n",
    "    metricsData = []\n",
    "    \n",
    "    for model, proportionResults in experimentResult.items():\n",
    "        for proportionStr, foldResults in proportionResults.items():\n",
    "            for fold, resultsDict in foldResults.items():\n",
    "                metrics = resultsDict[\"metrics\"]\n",
    "                for metricName, metricValue in metrics.items():\n",
    "                    metricsData.append([model, proportionStr, fold, metricName, float(metricValue)])\n",
    "\n",
    "    metricsColumns = [\"model\", \"proportion\", \"fold\", \"metric\", \"value\"]\n",
    "    metricsDF = pd.DataFrame(data = metricsData, columns = metricsColumns)\n",
    "\n",
    "    summaryDF = metricsDF.groupby([\"model\", \"proportion\", \"metric\"])[\"value\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "\n",
    "    for row in summaryDF.itertuples():\n",
    "        logger.info(f\"{row.model} | {row.proportion}: {row.metric} = {row.mean:.4f} +- {row.std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLearningCurve(experimentDict: dict, logger = None) -> dict:\n",
    "    \"\"\"Given a dict specifying the experiment setting, it runs the k-fold cross validation. At each iteration k, the k-th fold will be used to evaluate the model and the remaining will be used for training. The train set from the target domain is shuffled and divided into 5 subgroups. We progressively add each of them in the final train set, so generating a learning curve on the amount of target data available for training. This experiment is similar to that described in page 73 from Rodrigo's work [1].\n",
    "\n",
    "    [1] https://cos.ufrj.br/uploadfile/publicacao/2903.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    experiment = experimentDict\n",
    "    experimentID = experiment[\"id\"]\n",
    "    experimentBasePath = os.path.join(experiment[\"path\"], experimentID)\n",
    "\n",
    "    os.makedirs(experimentBasePath, exist_ok = True)\n",
    "\n",
    "    with open(os.path.join(experimentBasePath, \"setting.json\"), \"w\") as f:\n",
    "        json.dump(experiment, f)\n",
    "\n",
    "    if not logger:\n",
    "        logger = getLogger(experimentID, level = logging.DEBUG)\n",
    "\n",
    "    logger.info(\"Parsing experiment parameters...\")\n",
    "\n",
    "    useRecursion = experiment.get(\"useRecursion\", False)\n",
    "    negPosRatio = experiment.get(\"negPosRatio\", 1)\n",
    "    randomSeed = experiment.get(\"randomSeed\", 10)\n",
    "    maxFailedNegSamplingRetries = experiment.get(\"maxFailedNegSamplingRetries\", 50)\n",
    "    numberOfClauses = experiment.get(\"numberOfClauses\", 8)\n",
    "    numberOfCycles = experiment.get(\"numberOfCycles\", 100)\n",
    "    maxTreeDepth = experiment.get(\"maxTreeDepth\", 3)\n",
    "    nEstimators = experiment.get(\"nEstimators\", 10)\n",
    "    nodeSize = experiment.get(\"nodeSize\", 2)\n",
    "    sourceUtilityAlpha = experiment.get(\"sourceUtilityAlpha\", 1)\n",
    "    targetUtilityAlpha = experiment.get(\"targetUtilityAlpha\", 1)\n",
    "    utilityAlphaSetIter = experiment.get(\"utilityAlphaSetIter\", 1)\n",
    "    runOriginalRDNBoost = experiment.get(\"runOriginalRDNBoost\", False)\n",
    "    runTransferLearning = experiment.get(\"runTransferLearning\", False)\n",
    "    runAnalogousToOriginalRDNBoost = experiment.get(\"runAnalogousToOriginalRDNBoost\", False)\n",
    "    runTreeBoostler = experiment.get(\"runTreeBoostler\", False)\n",
    "    ignoreSTDOUT = experiment.get(\"ignoreSTDOUT\", False)\n",
    "    searchArgPermutation = experiment.get(\"searchArgPermutation\", True)\n",
    "    allowSameTargetMap = experiment.get(\"allowSameTargetMap\", False)\n",
    "    refine = experiment.get(\"refine\", True)\n",
    "    maxRevisionIterations = experiment.get(\"maxRevisionIterations\", 1)\n",
    "\n",
    "    anyModelIsSet = runOriginalRDNBoost or runTransferLearning or runAnalogousToOriginalRDNBoost or runTreeBoostler\n",
    "    assert anyModelIsSet, \"No model to run. `runOriginalRDNBoost`, `runTransferLearning`, and `analogousToOriginalRDNBoost` can not be set to False simultaneously.\"\n",
    "\n",
    "    if runTransferLearning or runTreeBoostler:\n",
    "        logger.info(\"Loading source database...\")\n",
    "        \n",
    "        sourceDatabase = loadDatabase(\n",
    "            folds = None, \n",
    "            useRecursion = useRecursion, \n",
    "            logger = logger,\n",
    "            negPosRatio = negPosRatio,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            **experiment[\"sourceDatabase\"]\n",
    "        )\n",
    "\n",
    "        if runTransferLearning:\n",
    "            weightFactory = WeightFactory()\n",
    "            weightStrategy = weightFactory.getWeightStrategy(experiment[\"weight\"][\"strategy\"], **experiment[\"weight\"][\"parameters\"])\n",
    "        \n",
    "            relationMapping = experiment[\"mapping\"][\"relationMapping\"]\n",
    "            termTypeMapping = experiment[\"mapping\"][\"termTypeMapping\"]\n",
    "\n",
    "    targetDatabasePath = experiment[\"targetDatabase\"][\"path\"]\n",
    "    allTargetFolds = [os.path.basename(path) for path in glob(f\"{targetDatabasePath}/fold*\")]\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for fold in allTargetFolds:\n",
    "        logger.info(f\"RUNNING EXPERIMENTS USING {fold.upper()} AS TESTING FOLD...\")\n",
    "\n",
    "        targetDatabaseTestFold = fold\n",
    "        targetDatabaseTrainFolds = list(set(allTargetFolds) - set([targetDatabaseTestFold]))\n",
    "\n",
    "        logger.info(\"Loading target database for testing...\")\n",
    "        logger.debug(f\"Test fold: {targetDatabaseTestFold}\")\n",
    "\n",
    "        targetDatabaseTest = loadDatabase(\n",
    "            folds = [targetDatabaseTestFold], \n",
    "            useRecursion = useRecursion, \n",
    "            logger = logger,\n",
    "            negPosRatio = negPosRatio,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            **experiment[\"targetDatabase\"]\n",
    "        )\n",
    "\n",
    "        logger.info(\"Loading target database for training...\")\n",
    "        logger.debug(f\"Train folds: {targetDatabaseTrainFolds}\")\n",
    "\n",
    "        targetDatabaseTrain = loadDatabase(\n",
    "            folds = targetDatabaseTrainFolds, \n",
    "            useRecursion = useRecursion, \n",
    "            logger = logger,\n",
    "            negPosRatio = negPosRatio,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            **experiment[\"targetDatabase\"]\n",
    "        )\n",
    "\n",
    "        targetDomainTargetRelation = targetDatabaseTrain.getTargetRelation()\n",
    "\n",
    "        logger.debug(f\"Target relation for target database: {targetDomainTargetRelation}\")\n",
    "\n",
    "        numTrainSplits = 5\n",
    "\n",
    "        # TODO: Implement random seed to guarantee reproducibility.\n",
    "        targetDatabaseTrainSplits = iter(Database.getKFolds(\n",
    "            targetDatabaseTrain, \n",
    "            numFolds = numTrainSplits,\n",
    "            shuffle = True\n",
    "        ))\n",
    "\n",
    "        targetDatabaseTrain = next(targetDatabaseTrainSplits)\n",
    "\n",
    "        for trainSplit in range(1, numTrainSplits + 1):\n",
    "            targetTrainProportion = (1/numTrainSplits)*trainSplit\n",
    "\n",
    "            logger.info(f\"Target train set proportion: {targetTrainProportion:.2f}\")\n",
    "\n",
    "            trainProportionStr = f\"trainProportion-{targetTrainProportion:.2f}\"\n",
    "            experimentFoldPath = os.path.join(experimentBasePath, fold, trainProportionStr)\n",
    "            os.makedirs(experimentFoldPath, exist_ok = True)\n",
    "\n",
    "            if runOriginalRDNBoost:\n",
    "                result[\"originalRDNBoost\"] = result.get(\"originalRDNBoost\", {})\n",
    "                result[\"originalRDNBoost\"][trainProportionStr] = result[\"originalRDNBoost\"].get(trainProportionStr, {})\n",
    "                result[\"originalRDNBoost\"][trainProportionStr][fold] = runSingleExperiment_OriginalRDNBoost(\n",
    "                    experimentPath = experimentFoldPath, \n",
    "                    databaseTrain = targetDatabaseTrain,\n",
    "                    databaseTest = targetDatabaseTest,\n",
    "                    nEstimators = nEstimators,\n",
    "                    nodeSize = nodeSize,\n",
    "                    maxTreeDepth = maxTreeDepth,\n",
    "                    negPosRatio = negPosRatio,\n",
    "                    numberOfClauses = numberOfClauses,\n",
    "                    numberOfCycles = numberOfCycles,\n",
    "                    ignoreSTDOUT = ignoreSTDOUT,\n",
    "                    logger = logger        \n",
    "                )\n",
    "\n",
    "            if runAnalogousToOriginalRDNBoost:\n",
    "                result[\"analogousToOriginalRDNBoost\"] = result.get(\"analogousToOriginalRDNBoost\", {})\n",
    "                result[\"analogousToOriginalRDNBoost\"][trainProportionStr] = result[\"analogousToOriginalRDNBoost\"].get(trainProportionStr, {})\n",
    "                result[\"analogousToOriginalRDNBoost\"][trainProportionStr][fold] = runSingleExperiment_AnalogousToRDNBoost(\n",
    "                    experimentPath = experimentFoldPath, \n",
    "                    databaseTrain = targetDatabaseTrain,\n",
    "                    databaseTest = targetDatabaseTest,\n",
    "                    nEstimators = nEstimators,\n",
    "                    nodeSize = nodeSize,\n",
    "                    maxTreeDepth = maxTreeDepth,\n",
    "                    negPosRatio = negPosRatio,\n",
    "                    numberOfClauses = numberOfClauses,\n",
    "                    numberOfCycles = numberOfCycles,\n",
    "                    ignoreSTDOUT = ignoreSTDOUT,\n",
    "                    logger = logger        \n",
    "                )\n",
    "\n",
    "            if runTransferLearning:\n",
    "                result[\"transferLearning\"] = result.get(\"transferLearning\", {})\n",
    "                result[\"transferLearning\"][trainProportionStr] = result[\"transferLearning\"].get(trainProportionStr, {})\n",
    "                result[\"transferLearning\"][trainProportionStr][fold] = runSingleExperiment_TransferLearning(\n",
    "                    experimentPath = experimentFoldPath,\n",
    "                    sourceDatabase = sourceDatabase,\n",
    "                    targetDatabaseTrain = targetDatabaseTrain,\n",
    "                    targetDatabaseTest = targetDatabaseTest,\n",
    "                    nEstimators = nEstimators,\n",
    "                    nodeSize = nodeSize,\n",
    "                    maxTreeDepth = maxTreeDepth,\n",
    "                    negPosRatio = negPosRatio,\n",
    "                    numberOfClauses = numberOfClauses,\n",
    "                    numberOfCycles = numberOfCycles,\n",
    "                    ignoreSTDOUT = ignoreSTDOUT,\n",
    "                    useRecursion = useRecursion,\n",
    "                    randomSeed = randomSeed,\n",
    "                    maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "                    weightStrategy = weightStrategy,\n",
    "                    sourceUtilityAlpha = sourceUtilityAlpha,\n",
    "                    targetUtilityAlpha = targetUtilityAlpha,\n",
    "                    utilityAlphaSetIter = utilityAlphaSetIter,\n",
    "                    relationMapping = relationMapping,\n",
    "                    termTypeMapping = termTypeMapping,\n",
    "                    logger = logger,\n",
    "                )\n",
    "\n",
    "            if runTreeBoostler:\n",
    "                result[\"treeBoostler\"] = result.get(\"treeBoostler\", {})\n",
    "                result[\"treeBoostler\"][trainProportionStr] = result[\"treeBoostler\"].get(trainProportionStr, {})\n",
    "                result[\"treeBoostler\"][trainProportionStr][fold] = runSingleExperiment_TreeBoostler(\n",
    "                    experimentPath = experimentFoldPath,\n",
    "                    sourceDatabase = sourceDatabase,\n",
    "                    targetDatabaseTrain = targetDatabaseTrain,\n",
    "                    targetDatabaseTest = targetDatabaseTest,\n",
    "                    nEstimators = nEstimators,\n",
    "                    nodeSize = nodeSize,\n",
    "                    maxTreeDepth = maxTreeDepth,\n",
    "                    negPosRatio = negPosRatio,\n",
    "                    numberOfClauses = numberOfClauses,\n",
    "                    numberOfCycles = numberOfCycles,\n",
    "                    ignoreSTDOUT = ignoreSTDOUT,\n",
    "                    searchArgPermutation = searchArgPermutation,\n",
    "                    allowSameTargetMap = allowSameTargetMap,\n",
    "                    refine = refine,\n",
    "                    maxRevisionIterations = maxRevisionIterations,\n",
    "                    logger = logger,\n",
    "                )\n",
    "            \n",
    "            if trainSplit < numTrainSplits:\n",
    "                targetDatabaseTrain = targetDatabaseTrain.merge(next(targetDatabaseTrainSplits))\n",
    "\n",
    "    metricsJSONPath = os.path.join(experimentBasePath, \"metrics.json\")\n",
    "    logger.info(f\"Storing performance metrics at {metricsJSONPath}.\")\n",
    "\n",
    "    allMetrics = {}\n",
    "    for model, foldsResults in result.items():\n",
    "        allMetrics[model] = {}\n",
    "        for fold, trainProportionsResults in foldsResults.items():\n",
    "            allMetrics[model][fold] = {}\n",
    "            for trainProportion, resultsDict in trainProportionsResults.items():\n",
    "                allMetrics[model][fold][trainProportion] = resultsDict[\"metrics\"]\n",
    "\n",
    "    with open(metricsJSONPath, \"w\") as f:\n",
    "        json.dump(allMetrics, f)\n",
    "\n",
    "    logger.info(\"Experiment has been finished.\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimentsLearningCurve = {}\n",
    "with open(\"experiments-learningCurve.json\") as f:\n",
    "    experimentsLearningCurve = json.load(f)\n",
    "totalExperimentsLearningCurve = len(experimentsLearningCurve)\n",
    "totalExperimentsLearningCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can be leveraged to prioritize experiments.\n",
    "def skipExperiment(experimentDict):\n",
    "    # The experiment has already been carried out.\n",
    "    experimentID = experimentDict[\"id\"]\n",
    "    experimentPath = experimentDict['path']\n",
    "    if os.path.exists(f\"{experimentPath}/{experimentID}/metrics.json\"):\n",
    "        return True, \"The experiment has already been carried out.\"\n",
    "    return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 1 # An int greater or equal to 1\n",
    "skippedExperiments = []\n",
    "experimentsToRun = []\n",
    "numProcesses = 3\n",
    "\n",
    "for i, experimentDict in enumerate(experimentsLearningCurve[start-1:], start = start):\n",
    "    # TODO: I am temporarily ignoring other models. I  need to further run the other experiments\n",
    "    shouldSkipExperiment, skipMessage = skipExperiment(experimentDict)\n",
    "    experimentID = experimentDict[\"id\"]\n",
    "    if shouldSkipExperiment:\n",
    "        skippedExperiments.append((experimentID, skipMessage))\n",
    "    else:\n",
    "        experimentsToRun.append(experimentDict)\n",
    "\n",
    "totalExperimentsToRun = len(experimentsToRun)\n",
    "totalExperimentsToRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsRunningMode = \"parallel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment 816f88e322ad9332974b410b99743c591cfb28291770844b23d5ab2e50cf7967...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment finished successfully: 816f88e322ad9332974b410b99743c591cfb28291770844b23d5ab2e50cf7967...\n"
     ]
    }
   ],
   "source": [
    "# Parallel execution of the experiments. \n",
    "# TODO: We get a Kernel Crash and it occurs only when we import the models from srlearn.rdn under Python 3.8.10. In our tests, this problem is solved when running over Python 3.10.5, but the reason why it does not works under Python 3.8.10 is still unknown.\n",
    " \n",
    "if experimentsRunningMode == \"parallel\":\n",
    "    def safePrint(message, consoleOutputLock: multiprocessing.managers.AcquirerProxy):\n",
    "        consoleOutputLock.acquire()\n",
    "        print(message)\n",
    "        consoleOutputLock.release()\n",
    "\n",
    "    def experimentWorker(experimentDict: dict, consoleOutputLock: multiprocessing.managers.AcquirerProxy):\n",
    "        experimentID = experimentDict[\"id\"]\n",
    "        safePrint(f\"Starting experiment {experimentID}...\", consoleOutputLock)\n",
    "        try:\n",
    "            experimentPath = f\"{experimentDict['path']}/{experimentID}\"\n",
    "            os.makedirs(experimentPath, exist_ok = True)    \n",
    "            logger = getLogger(experimentID, logFile = f\"{experimentPath}/experiment.log\", consoleOutput = False)\n",
    "            logger.info(f\"RUNNING EXPERIMENT {i}/{totalExperimentsToRun}...\")\n",
    "            experimentResult = runLearningCurve(experimentDict, logger = logger)\n",
    "            experimentResultSummarization(experimentResult, logger = logger)\n",
    "            safePrint(f\"Experiment finished successfully: {experimentID}...\", consoleOutputLock)\n",
    "        except Exception as e:\n",
    "            safePrint(f\"The following exception was raised while running the experiment {experimentID}: {e}. Check the logs in the experiment directory for more details.\", consoleOutputLock)\n",
    "            raise e\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers = 8) as p:\n",
    "        with multiprocessing.Manager() as manager:\n",
    "            consoleOutputLock = manager.Lock()\n",
    "            futures = p.map(experimentWorker, experimentsToRun, [consoleOutputLock for experiment in experimentsToRun])\n",
    "            for result in futures:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential execution of the experiments. \n",
    "if experimentsRunningMode == \"sequential\":\n",
    "    totalExperimentsToRun = len(experimentsToRun)\n",
    "    for i, experimentDict in enumerate(experimentsToRun, start = 1):\n",
    "        experimentID = experimentDict[\"id\"]\n",
    "        experimentPath = f\"{experimentDict['path']}/{experimentID}\"\n",
    "        os.makedirs(experimentPath, exist_ok = True)    \n",
    "        logger = getLogger(experimentID, logFile = f\"{experimentPath}/experiment.log\")\n",
    "        logger.info(f\"RUNNING EXPERIMENT {i}/{totalExperimentsToRun}...\")\n",
    "        experimentResult = runLearningCurve(experimentDict, logger = logger)\n",
    "        experimentResultSummarization(experimentResult, logger = logger)\n",
    "        clear_output(wait = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
