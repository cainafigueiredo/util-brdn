{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a generator for both source and target datasets from an existing dataset of a single domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Databases literals are converted to arity 2 in order to easily work with graphs;\n",
    "- Train and test split is performed based on the original folds. One fold is used for test and the other ones are used for training;\n",
    "- Train set is split into 5 folds. Four of them are used as the source dataset and other is used for the target one;\n",
    "- Noise is added to the source dataset and includes adding, removing or modifying the type of edges;\n",
    "- Negative examples from the source dataset are resampled after adding noise;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRLEARN_PATH = \"../../srlearn\"\n",
    "PROJECT_PATH = \"..\"\n",
    "import sys\n",
    "sys.path.append(SRLEARN_PATH)\n",
    "sys.path.append(PROJECT_PATH)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import functools\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from itertools import combinations\n",
    "from srlearn.database import Database\n",
    "from srlearn.weight import WeightFactory\n",
    "\n",
    "from utils.experiment import loadDatabase, getLogger, runSingleExperiment_TransferLearning\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing.managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addGraphs(G1, G2):\n",
    "    # This method assumes that:\n",
    "    #   - V1 (set of nodes from G1) is a subset of V2 (set of nodes from G2)\n",
    "    #   - Each node has the same id in both graphs\n",
    "    #   - Each edge has an attribute named `edgeType`\n",
    "    # It is inpired by the following paper: https://link.springer.com/chapter/10.1007/978-3-030-31500-9_8\n",
    "    #   - Adding edges: an edge from G1 does not exist in G2\n",
    "    #   - Edges removal: an edge from G1 shares the same type as the corresponding edge from G2\n",
    "    #   - Edge type modification: an edge type from G1 is different from the corresponding edge in G2 \n",
    "    # Due to Edge type modification, (G1 + G2) != (G2 + G1)\n",
    "\n",
    "    G = G2.copy()\n",
    "\n",
    "    for edge in G1.edges:\n",
    "        # Adding edges\n",
    "        if edge not in G2.edges:\n",
    "            edgeType = G1.edges[edge][\"edgeType\"]\n",
    "            G.add_edge(*edge, edgeType = edgeType)\n",
    "        \n",
    "        elif edge in G2.edges:\n",
    "            edgeTypeG1 = G1.edges[edge][\"edgeType\"]\n",
    "            edgeTypeG2 = G2.edges[edge][\"edgeType\"]\n",
    "            \n",
    "            # Edges removal\n",
    "            if edgeTypeG1 == edgeTypeG2:\n",
    "                G.remove_edge(*edge)\n",
    "            \n",
    "            # Edge type modification\n",
    "            else:\n",
    "                G.edges[edge][\"edgeType\"] = edgeTypeG1\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentResultSummarization(experimentResult: dict, logger = None, ):\n",
    "    if not logger:\n",
    "        logger = getLogger(\"Result summarization\")\n",
    "\n",
    "    logger.info(\"Extracting performance metrics from experiment results:\")\n",
    "    metrics = {}\n",
    "    for exp, expResults in experimentResult.items():\n",
    "        metrics[exp] = metrics.get(exp, {})\n",
    "        for trainFold, foldResults in expResults.items():\n",
    "            for metricName, metricValue in foldResults[\"metrics\"].items():\n",
    "                metrics[exp][metricName] = metrics[exp].get(metricName, [])\n",
    "                metrics[exp][metricName] += [float(metricValue)]\n",
    "\n",
    "    for exp, expMetrics in metrics.items():\n",
    "        for metricName, metricValues in expMetrics.items():\n",
    "            metricValues = np.array(metricValues)\n",
    "            mean = metricValues.mean()\n",
    "            std = metricValues.std()\n",
    "            logger.info(f\"{exp}: {metricName} = {mean:.4f} +- {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runNoiseExperiment(\n",
    "    experimentDict: dict, logger = None\n",
    "):\n",
    "    experiment = experimentDict\n",
    "    experimentID = experiment[\"id\"]\n",
    "    experimentBasePath = os.path.join(experiment[\"path\"], experimentID)\n",
    "\n",
    "    os.makedirs(experimentBasePath, exist_ok = True)\n",
    "\n",
    "    with open(os.path.join(experimentBasePath, \"setting.json\"), \"w\") as f:\n",
    "        json.dump(experiment, f)\n",
    "\n",
    "    if not logger:\n",
    "        logger = getLogger(experimentID, level = logging.DEBUG)\n",
    "\n",
    "    logger.info(\"Parsing experiment parameters...\")\n",
    "\n",
    "    randomSeed = experiment.get(\"randomSeed\", RANDOM_SEED)\n",
    "    useRecursion = experiment.get(\"useRecursion\", False)\n",
    "    negPosRatio = experiment.get(\"negPosRatio\", 1)\n",
    "    maxFailedNegSamplingRetries = experiment.get(\"maxFailedNegSamplingRetries\", 50)\n",
    "    nEstimators = experiment.get(\"nEstimators\", 10)\n",
    "    nodeSize = experiment.get(\"nodeSize\", 2)\n",
    "    maxTreeDepth = experiment.get(\"maxTreeDepth\", 3)\n",
    "    numberOfClauses = experiment.get(\"numberOfClauses\", 8)\n",
    "    numberOfCycles = experiment.get(\"numberOfCycles\", 100)\n",
    "    ignoreSTDOUT = experiment.get(\"ignoreSTDOUT\", True)\n",
    "    weightFactory = WeightFactory()\n",
    "    weightStrategy = weightFactory.getWeightStrategy(\n",
    "        experiment[\"weight\"][\"strategy\"], \n",
    "        **experiment[\"weight\"][\"parameters\"]\n",
    "    )\n",
    "    sourceUtilityAlpha = experiment.get(\"sourceUtilityAlpha\", 1)\n",
    "    targetUtilityAlpha = experiment.get(\"targetUtilityAlpha\", 1)\n",
    "    utilityAlphaSetIter = experiment.get(\"utilityAlphaSetIter\", 1)\n",
    "\n",
    "    trainNSplits = experiment.get(\"trainNSplits\", 5) # The train set is split into folds. Some of them are used as source set and the remaing are used as target set.\n",
    "    trainSourceSplits = experiment.get(\"trainSourceSplits\", trainNSplits - 1) # Number of train folds to be used as source. \n",
    "    trainTargetSplits = trainNSplits - trainSourceSplits # Number of train folds to be used as target.\n",
    "    noiseStrength = experiment.get(\"noiseStrength\", 0.005) # Probability of adding an edge between a given pair of nodes\n",
    "\n",
    "    datasetPath = experiment.get(\"databasePath\")\n",
    "    targetPredicate = experiment.get(\"targetPredicate\", None)\n",
    "    resetTargetPredicate = experiment.get(\"resetTargetPredicate\", None)\n",
    "    datasetFolds = [os.path.basename(foldPath) for foldPath in glob(f\"{datasetPath}/fold*\")]\n",
    "\n",
    "    np.random.seed(randomSeed)\n",
    "\n",
    "    result = {}\n",
    "    for testFold in datasetFolds:\n",
    "        experimentFoldPath = os.path.join(experimentBasePath, testFold)\n",
    "        os.makedirs(experimentFoldPath, exist_ok = True)\n",
    "\n",
    "        logger.info(f\"RUNNING EXPERIMENTS USING {testFold.upper()} AS TEST FOLD...\")\n",
    "\n",
    "        logger.info(f\"Loading test database and converting it to arity 2...\")\n",
    "        testDataset = loadDatabase(\n",
    "            path = datasetPath,\n",
    "            folds = [testFold],\n",
    "            useRecursion = useRecursion,\n",
    "            targetPredicate = targetPredicate,\n",
    "            resetTargetPredicate = resetTargetPredicate, \n",
    "            negPosRatio = negPosRatio,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            logger = logger\n",
    "        )\n",
    "        testDataset = Database.convertDatabaseToArity2(testDataset)\n",
    "\n",
    "        logger.info(\"Loading database for training and converting it to arity 2...\")\n",
    "\n",
    "        trainFolds = [fold for fold in datasetFolds if fold != testFold]\n",
    "        trainDataset = loadDatabase(\n",
    "            path = datasetPath,\n",
    "            folds = trainFolds,\n",
    "            useRecursion = useRecursion,\n",
    "            targetPredicate = targetPredicate,\n",
    "            resetTargetPredicate = resetTargetPredicate, \n",
    "            negPosRatio = negPosRatio,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            logger = logger\n",
    "        )\n",
    "        trainDataset = Database.convertDatabaseToArity2(trainDataset)\n",
    "\n",
    "        logger.debug(f\"Splitting train database into source {trainSourceSplits/trainNSplits*100:.0f}% and target {trainTargetSplits/trainNSplits*100:.0f}%...\")\n",
    "\n",
    "        trainDatasetSplits = list(Database.getKFolds(trainDataset, numFolds = trainNSplits, shuffle = True))\n",
    "        trainDatasetTarget = functools.reduce(lambda merged, split: merged.merge(split), trainDatasetSplits[:trainTargetSplits])\n",
    "        trainDatasetSource = functools.reduce(lambda merged, split: merged.merge(split), trainDatasetSplits[trainTargetSplits:])\n",
    "        schema = trainDatasetSource.extractSchemaPreds()\n",
    "\n",
    "        candidateRelationTypes = {} \n",
    "        for relation, nodeTypes in schema.items():\n",
    "            sortedNodeTypes = tuple(sorted(nodeTypes))\n",
    "            candidates = candidateRelationTypes.get(sortedNodeTypes, [])\n",
    "            candidateRelationTypes[sortedNodeTypes] = candidates + [relation]\n",
    "\n",
    "        trainGraphSource = Database.convertDatabaseToUndirectedGraph(trainDatasetSource)\n",
    "\n",
    "        logger.debug(f\"Adding noise to source database (noiseStrength: {noiseStrength})...\")\n",
    "\n",
    "        noiseGraph = nx.Graph()\n",
    "        noiseGraph.add_nodes_from(trainGraphSource.nodes(data = True))\n",
    "\n",
    "        potentialEdges = combinations(trainGraphSource.nodes, 2)\n",
    "\n",
    "        for edge in potentialEdges:\n",
    "            if np.random.rand() < noiseStrength:\n",
    "                nodeA, nodeB = edge\n",
    "                nodeAType = noiseGraph.nodes[nodeA][\"nodeType\"]\n",
    "                nodeBType = noiseGraph.nodes[nodeB][\"nodeType\"]\n",
    "                sortedNodeTypes = tuple(sorted([nodeAType, nodeBType]))\n",
    "                relationCandidates = candidateRelationTypes.get(sortedNodeTypes)\n",
    "                if relationCandidates:\n",
    "                    sampledCandidate = np.random.choice(relationCandidates, 1)[0]\n",
    "                    noiseGraph.add_edge(*edge, edgeType = sampledCandidate)\n",
    "\n",
    "        logger.debug(f\"Combining target and noisy source databases...\")\n",
    "\n",
    "        noiseTrainGraphSource = addGraphs(noiseGraph, trainGraphSource)\n",
    "\n",
    "        noiseTrainDatasetSource = Database.populateFromGraph(\n",
    "            graph = noiseTrainGraphSource,\n",
    "            modes = trainDatasetSource.modes,\n",
    "            targetRelation = trainDatasetSource.getTargetRelation(),\n",
    "            useRecursion = useRecursion,\n",
    "            negPosRatio = negPosRatio\n",
    "        )\n",
    "\n",
    "        relationMapping = {}\n",
    "        termTypeMapping = {}\n",
    "        for relationType, termTypes in schema.items():\n",
    "            relationMapping[relationType] = relationType\n",
    "            for termType in termTypes:\n",
    "                termTypeMapping[termType] = termType\n",
    "\n",
    "        logger.debug(f\"Training and evaluating model...\")\n",
    "\n",
    "        result[testFold] = runSingleExperiment_TransferLearning(\n",
    "            experimentPath = experimentFoldPath, \n",
    "            sourceDatabase = noiseTrainDatasetSource,\n",
    "            targetDatabaseTrain = trainDatasetTarget,\n",
    "            targetDatabaseTest = testDataset,\n",
    "            nEstimators = nEstimators,\n",
    "            nodeSize = nodeSize,\n",
    "            maxTreeDepth = maxTreeDepth,\n",
    "            negPosRatio = negPosRatio,\n",
    "            numberOfClauses = numberOfClauses,\n",
    "            numberOfCycles = numberOfCycles,\n",
    "            ignoreSTDOUT = ignoreSTDOUT,\n",
    "            useRecursion = useRecursion,\n",
    "            randomSeed = RANDOM_SEED,\n",
    "            maxFailedNegSamplingRetries = maxFailedNegSamplingRetries,\n",
    "            weightStrategy = weightStrategy,\n",
    "            sourceUtilityAlpha = sourceUtilityAlpha,\n",
    "            targetUtilityAlpha = targetUtilityAlpha,\n",
    "            utilityAlphaSetIter = utilityAlphaSetIter,\n",
    "            relationMapping = relationMapping,\n",
    "            termTypeMapping = termTypeMapping,\n",
    "            logger = logger\n",
    "        )\n",
    "\n",
    "    metricsJSONPath = os.path.join(experimentBasePath, \"metrics.json\")\n",
    "    logger.info(f\"Storing performance metrics at {metricsJSONPath}.\")\n",
    "\n",
    "    allMetrics = {}\n",
    "    for fold, foldResults in result.items():\n",
    "        allMetrics[fold] = foldResults[\"metrics\"]\n",
    "\n",
    "    with open(metricsJSONPath, \"w\") as f:\n",
    "        json.dump(allMetrics, f)\n",
    "\n",
    "    logger.info(\"Experiment has been finished.\")\n",
    "        \n",
    "    return {\"transferLearning\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2625"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = {}\n",
    "with open(\"experiments-noisyTransferLearning.json\") as f:\n",
    "    experiments = json.load(f)\n",
    "totalExperiments = len(experiments)\n",
    "totalExperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can be leveraged to prioritize experiments.\n",
    "def skipExperiment(experimentDict):\n",
    "    # The experiment has already been carried out.\n",
    "    experimentID = experimentDict[\"id\"]\n",
    "    experimentPath = experimentDict['path']\n",
    "    if os.path.exists(f\"{experimentPath}/{experimentID}/metrics.json\"):\n",
    "        return True, \"The experiment has already been carried out.\"\n",
    "    return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 1 # An int greater or equal to 1\n",
    "skippedExperiments = []\n",
    "experimentsToRun = []\n",
    "numProcesses = 3\n",
    "\n",
    "for i, experimentDict in enumerate(experiments[start-1:], start = start):\n",
    "    shouldSkipExperiment, skipMessage = skipExperiment(experimentDict)\n",
    "    experimentID = experimentDict[\"id\"]\n",
    "    if shouldSkipExperiment:\n",
    "        skippedExperiments.append((experimentID, skipMessage))\n",
    "    else:\n",
    "        os.system(f\"rm -rf {experimentDict['path']}/{experimentID}\")\n",
    "        experimentsToRun.append(experimentDict)\n",
    "\n",
    "len(experimentsToRun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentsRunningMode = \"parallel\" # Either \"parallel\" or \"sequential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel execution of the experiments. \n",
    "# TODO: We get a Kernel Crash and it occurs only when we import the models from srlearn.rdn under Python 3.8.10. In our tests, this problem is solved when running over Python 3.10.5, but the reason why it does not works under Python 3.8.10 is still unknown.\n",
    " \n",
    "if experimentsRunningMode == \"parallel\":\n",
    "    def safePrint(message, consoleOutputLock: multiprocessing.managers.AcquirerProxy):\n",
    "        consoleOutputLock.acquire()\n",
    "        print(message)\n",
    "        consoleOutputLock.release()\n",
    "\n",
    "    def experimentWorker(experimentDict: dict, consoleOutputLock: multiprocessing.managers.AcquirerProxy):\n",
    "        experimentID = experimentDict[\"id\"]\n",
    "        safePrint(f\"Starting experiment {experimentID}...\", consoleOutputLock)\n",
    "        try:\n",
    "            experimentPath = f\"{experimentDict['path']}/{experimentID}\"\n",
    "            os.makedirs(experimentPath, exist_ok = True)    \n",
    "            logger = getLogger(experimentID, logFile = f\"{experimentPath}/experiment.log\", consoleOutput = False)\n",
    "            logger.info(f\"RUNNING EXPERIMENT {i}/{totalExperiments}...\")\n",
    "            experimentResult = runNoiseExperiment(experimentDict, logger = logger)\n",
    "            experimentResultSummarization(experimentResult, logger = logger)\n",
    "            safePrint(f\"Experiment finished successfully: {experimentID}...\", consoleOutputLock)\n",
    "        except Exception as e:\n",
    "            safePrint(f\"The following exception was raised while running the experiment {experimentID}: {e}. Check the logs in the experiment directory for more details.\", consoleOutputLock)\n",
    "            raise e\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers = 8) as p:\n",
    "        with multiprocessing.Manager() as manager:\n",
    "            consoleOutputLock = manager.Lock()\n",
    "            futures = p.map(experimentWorker, experimentsToRun, [consoleOutputLock for experiment in experimentsToRun])\n",
    "            for result in futures:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential execution of the experiments. \n",
    "if experimentsRunningMode == \"sequential\":\n",
    "    totalExperimentsToRun = len(experimentsToRun)\n",
    "    for i, experimentDict in enumerate(experimentsToRun, start = 1):\n",
    "        experimentID = experimentDict[\"id\"]\n",
    "        experimentPath = f\"{experimentDict['path']}/{experimentID}\"\n",
    "        os.makedirs(experimentPath, exist_ok = True)    \n",
    "        logger = getLogger(experimentID, logFile = f\"{experimentPath}/experiment.log\")\n",
    "        logger.info(f\"RUNNING EXPERIMENT {i}/{totalExperimentsToRun}...\")\n",
    "        experimentResult = runNoiseExperiment(experimentDict, logger = logger)\n",
    "        experimentResultSummarization(experimentResult, logger = logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
